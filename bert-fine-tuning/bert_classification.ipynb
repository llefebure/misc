{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert-classification",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lw2UzDjItGP",
        "colab_type": "text"
      },
      "source": [
        "# Fine Tuning Bert for Tweet Classification\n",
        "\n",
        "This notebook goes through an example of fine tuning BERT for text classification using the `transformers` library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FtoxTBaIr7y",
        "colab_type": "code",
        "outputId": "8598ab20-69e5-49b3-c17c-3de8a059cc00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 799kB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.47)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 1.3MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 1.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.47)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=153c5be25500aaa6726708d83a7d74748e339329f8d48583d89d02d4909adb47\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 transformers-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MixPYvKOAjXH",
        "colab_type": "code",
        "outputId": "c7219d30-1df4-45cc-db71-5442a6f021de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import (\n",
        "    TensorDataset,\n",
        "    DataLoader,\n",
        "    RandomSampler,\n",
        "    SequentialSampler\n",
        ")\n",
        "from tqdm import tqdm\n",
        "from transformers import (\n",
        "    BertTokenizer,\n",
        "    BertForSequenceClassification,\n",
        "    AdamW,\n",
        "    BertConfig,\n",
        "    get_linear_schedule_with_warmup\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5cPMjk8AYPa",
        "colab_type": "text"
      },
      "source": [
        "## Fetch and Process Data\n",
        "\n",
        "In this example, I'll use the \"Disasters on Social Media\" dataset from [Figure Eight](https://www.figure-eight.com/data-for-everyone/).\n",
        "\n",
        "> Contributors looked at over 10,000 tweets culled with a variety of searches like “ablaze”, “quarantine”, and “pandemonium”, then noted whether the tweet referred to a disaster event (as opposed to a joke with the word or a movie review or something non-disastrous).\n",
        "\n",
        "The classification task is to predict whether a tweet refers to an actual disaster event or not.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP-axyNG_lJ4",
        "colab_type": "code",
        "outputId": "aaa013a1-73ba-4395-8464-329e63e40e30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "! curl -o disaster-tweets.csv https://d1p17r2m4rzlbo.cloudfront.net/wp-content/uploads/2016/03/socialmedia-disaster-tweets-DFE.csv"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 2156k  100 2156k    0     0  6781k      0 --:--:-- --:--:-- --:--:-- 6781k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7L8yVXJAe72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('disaster-tweets.csv', encoding='ISO-8859-1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRY4OfUh0pm3",
        "colab_type": "text"
      },
      "source": [
        "To use the BERT pretrained model, we need to use the same tokenizer that was used for initial training. `tokenizer.encode` below properly encodes a text input sequence to a list of tokens (including special sentence start and end tokens). `Z` here is a mask identifying which tokens in the input are actual tokens versus padding. This is a necessary input to the BERT model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9NuNtfZGP-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "X = [tokenizer.encode(text) for text in data.text]\n",
        "X = pad_sequences(X, padding='post', maxlen=128)\n",
        "y = np.array(data.choose_one == 'Relevant').astype(np.int)\n",
        "Z = (X != 0).astype(np.int)\n",
        "\n",
        "X_train, X_test, y_train, y_test, Z_train, Z_test = \\\n",
        "    train_test_split(X, y, Z, test_size=0.1, random_state=2018)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMBcsVOw4VIL",
        "colab_type": "text"
      },
      "source": [
        "## Model Building and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSoNMgbwK-BO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fine_tune_bert(\n",
        "    X_train, X_test, y_train, y_test, Z_train, Z_test, lr=2e-5, batch_size=32,\n",
        "    epochs=3, freeze_bert_layers=False\n",
        "):\n",
        "  train_data = TensorDataset(*map(torch.tensor, (X_train, Z_train, y_train)))\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(\n",
        "      train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  validation_data = TensorDataset(*map(torch.tensor, (X_test, Z_test, y_test)))\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(\n",
        "      validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "  model = BertForSequenceClassification.from_pretrained(\n",
        "      'bert-base-uncased',\n",
        "      num_labels=2,\n",
        "      output_attentions=False,\n",
        "      output_hidden_states=False\n",
        "  )\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "      print(\"Using GPU\")\n",
        "      model.cuda()\n",
        "      device = torch.device(\"cuda\")\n",
        "  else:\n",
        "      device = torch.device(\"cpu\")\n",
        "\n",
        "  if freeze_bert_layers:\n",
        "    for param in model.bert.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "  optimizer = AdamW(\n",
        "      model.parameters(),\n",
        "      lr=lr,\n",
        "      eps=1e-8\n",
        "  )\n",
        "\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "  scheduler = get_linear_schedule_with_warmup(\n",
        "      optimizer,\n",
        "      num_warmup_steps=0,\n",
        "      num_training_steps=total_steps\n",
        "  )\n",
        "\n",
        "  loss_values = []\n",
        "  for epoch_i in range(0, epochs):\n",
        "      total_loss = 0\n",
        "      model.train()\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "          seq, mask, labels = (x.to(device) for x in batch)\n",
        "          model.zero_grad()\n",
        "          outputs = model(\n",
        "              seq.to(torch.int64),\n",
        "              token_type_ids=None,\n",
        "              attention_mask=mask.to(torch.int64),\n",
        "              labels=labels.to(torch.int64)\n",
        "          )\n",
        "          loss = outputs[0]\n",
        "          total_loss += loss.item()\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "          optimizer.step()\n",
        "          scheduler.step()\n",
        "\n",
        "      # Calculate the average loss over the training data.\n",
        "      avg_train_loss = total_loss / len(train_dataloader)\n",
        "      print(f\"Epoch {epoch_i}\")\n",
        "      print(f\"--Training Loss: {avg_train_loss}\")\n",
        "\n",
        "      model.eval()\n",
        "\n",
        "      total_tp = 0\n",
        "      val_total = 0\n",
        "      for batch in validation_dataloader:\n",
        "          seq, mask, labels = (t.to(device) for t in batch)\n",
        "          with torch.no_grad():        \n",
        "              outputs = model(\n",
        "                  seq.to(torch.int64),\n",
        "                  token_type_ids=None,\n",
        "                  attention_mask=mask.to(torch.int64)\n",
        "              )\n",
        "          logits = outputs[0]\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          labels = labels.to('cpu').numpy()\n",
        "          val_total += logits.shape[0]\n",
        "          preds = np.argmax(logits, axis=1)\n",
        "          batch_tp = (preds == labels.flatten()).sum()\n",
        "          total_tp += batch_tp\n",
        "\n",
        "      # Report the final accuracy for this validation run.\n",
        "      print(\"--Validation Accuracy: {0:.2f}\".format(total_tp / val_total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNylYGNH4GIR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "d01f4061-34d7-4a64-ecb2-68c6b57760b2"
      },
      "source": [
        "fine_tune_bert(X_train, X_test, y_train, y_test, Z_train, Z_test, epochs=10)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "Epoch 0\n",
            "--Training Loss: 0.4368228014388116\n",
            "--Validation Accuracy: 0.85\n",
            "Epoch 1\n",
            "--Training Loss: 0.32688747726234735\n",
            "--Validation Accuracy: 0.85\n",
            "Epoch 2\n",
            "--Training Loss: 0.2518084531564728\n",
            "--Validation Accuracy: 0.84\n",
            "Epoch 3\n",
            "--Training Loss: 0.18333846961872446\n",
            "--Validation Accuracy: 0.82\n",
            "Epoch 4\n",
            "--Training Loss: 0.13055380151357526\n",
            "--Validation Accuracy: 0.83\n",
            "Epoch 5\n",
            "--Training Loss: 0.1028647772858248\n",
            "--Validation Accuracy: 0.84\n",
            "Epoch 6\n",
            "--Training Loss: 0.08213668294689234\n",
            "--Validation Accuracy: 0.83\n",
            "Epoch 7\n",
            "--Training Loss: 0.0686279149068629\n",
            "--Validation Accuracy: 0.83\n",
            "Epoch 8\n",
            "--Training Loss: 0.05718729714406472\n",
            "--Validation Accuracy: 0.82\n",
            "Epoch 9\n",
            "--Training Loss: 0.04954418827006533\n",
            "--Validation Accuracy: 0.82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l5GU4Eg4KT6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "eb52e0bb-0b8a-4e90-c795-bad63d362900"
      },
      "source": [
        "fine_tune_bert(\n",
        "    X_train, X_test, y_train, y_test, Z_train, Z_test, freeze_bert_layers=True,\n",
        "    epochs=10)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "Epoch 0\n",
            "--Training Loss: 0.6846879298001333\n",
            "--Validation Accuracy: 0.60\n",
            "Epoch 1\n",
            "--Training Loss: 0.6483962861151477\n",
            "--Validation Accuracy: 0.67\n",
            "Epoch 2\n",
            "--Training Loss: 0.6384691331121657\n",
            "--Validation Accuracy: 0.67\n",
            "Epoch 3\n",
            "--Training Loss: 0.6335444050092324\n",
            "--Validation Accuracy: 0.68\n",
            "Epoch 4\n",
            "--Training Loss: 0.6295321819439433\n",
            "--Validation Accuracy: 0.68\n",
            "Epoch 5\n",
            "--Training Loss: 0.6276212239187527\n",
            "--Validation Accuracy: 0.68\n",
            "Epoch 6\n",
            "--Training Loss: 0.6220013259672651\n",
            "--Validation Accuracy: 0.69\n",
            "Epoch 7\n",
            "--Training Loss: 0.6208711312292448\n",
            "--Validation Accuracy: 0.69\n",
            "Epoch 8\n",
            "--Training Loss: 0.6209131724694196\n",
            "--Validation Accuracy: 0.69\n",
            "Epoch 9\n",
            "--Training Loss: 0.6206710254834369\n",
            "--Validation Accuracy: 0.69\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp7eZFrc5pD2",
        "colab_type": "text"
      },
      "source": [
        "## Comparison with Logistic Regression\n",
        "\n",
        "Let's see how BERT compares with plain logistic regression on this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz_RTjXGTFGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnW53ZjBTIg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_model = LogisticRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6R6JeY1Th_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vec = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False, max_df=.75)\n",
        "X_train_tfidf = vec.fit_transform(X_train)\n",
        "X_test_tfidf = vec.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayvsP0tET-sC",
        "colab_type": "code",
        "outputId": "e28097d5-aa2c-47cb-8bf3-81352c51bd9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "lr_model.fit(X_train_tfidf, y_train)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vW3QM0xUUvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = lr_model.predict(X_test_tfidf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmiaLI5AUYZb",
        "colab_type": "code",
        "outputId": "f6b67da0-b4be-41b6-98a2-f99782101245",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f\"Logistic Regression Validation Accuracy: {(preds == y_test).mean()}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression Validation Accuracy: 0.8005514705882353\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}